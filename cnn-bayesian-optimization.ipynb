{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, BatchNormalization, MaxPooling2D, Flatten, Activation\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "def get_input_datasets(use_bfloat16=False):\n",
    "    \"\"\"Downloads the MNIST dataset and creates train and eval dataset objects.\n",
    "\n",
    "    Args:\n",
    "      use_bfloat16: Boolean to determine if input should be cast to bfloat16\n",
    "\n",
    "    Returns:\n",
    "      Train dataset, eval dataset and input shape.\n",
    "\n",
    "    \"\"\"\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 28, 28\n",
    "    cast_dtype = tf.bfloat16 if use_bfloat16 else tf.float32\n",
    "\n",
    "    # the data, split between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "    # train dataset\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_ds = train_ds.repeat()\n",
    "    train_ds = train_ds.map(lambda x, y: (tf.cast(x, cast_dtype), y))\n",
    "    train_ds = train_ds.batch(64, drop_remainder=True)\n",
    "\n",
    "    # eval dataset\n",
    "    eval_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    eval_ds = eval_ds.repeat()\n",
    "    eval_ds = eval_ds.map(lambda x, y: (tf.cast(x, cast_dtype), y))\n",
    "    eval_ds = eval_ds.batch(64, drop_remainder=True)\n",
    "\n",
    "    return train_ds, eval_ds, input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape, dropout2_rate=0.5):\n",
    "   \n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 28, 28\n",
    "\n",
    "    \n",
    "    # Define a CNN model to recognize MNIST.\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape, name=\"conv2d_1\"))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', name=\"conv2d_2\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name=\"maxpool2d_1\"))\n",
    "    model.add(Dropout(0.25, name=\"dropout_1\"))\n",
    "    model.add(Flatten(name=\"flatten\"))\n",
    "    model.add(Dense(128, activation='relu', name=\"dense_1\"))\n",
    "    model.add(Dropout(dropout2_rate, name=\"dropout_2\"))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax', name=\"dense_2\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, eval_ds, input_shape = get_input_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_with(input_shape, verbose, dropout2_rate, lr):\n",
    "\n",
    "    # Create the model using a specified hyperparameters.\n",
    "    model = get_model(input_shape, dropout2_rate)\n",
    "\n",
    "    # Train the model for a specified number of epochs.\n",
    "    optimizer= Adam(lr = lr)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss    = 'mse', \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    # Train the model with the train dataset.\n",
    "    model.fit(train_ds,\n",
    "              validation_data  = eval_ds,\n",
    "              epochs           = 5,\n",
    "              validation_steps = 60000 // 32,  \n",
    "              steps_per_epoch  = 60000 // 32, \n",
    "              verbose          = verbose)\n",
    "\n",
    "    # Evaluate the model with the eval dataset.\n",
    "    score = model.evaluate(eval_ds, steps = 10, verbose=1)\n",
    "    \n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print('\\n')\n",
    "\n",
    "    # Return the accuracy.\n",
    "\n",
    "    return score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "input_shape = input_shape\n",
    "verbose     = 1\n",
    "\n",
    "fit_with_partial = partial(fit_with, input_shape, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BayesianOptimization object will work out of the box without much tuning needed. The main method you should be aware of is maximize, which does exactly what you think it does.\n",
    "\n",
    "There are many parameters you can pass to maximize, nonetheless, the most important ones are:\n",
    "\n",
    "n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n",
    "init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounded region of parameter space\n",
    "\n",
    "bounds = {'lr'           :(1e-4, 1e-2),\n",
    "          'dropout2_rate':(0.05, 0.5),\n",
    "          'batch_size'   :(1, 4.001),\n",
    "          'num_filters'  :(1, 4.001),\n",
    "          'kernel_size'  :(2, 4.001)}\n",
    "          \n",
    "\n",
    "bounds_2 = {'dropout2_rate': (0.1, 0.5), \n",
    "            'lr'           : (1e-4, 1e-2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | dropou... |    lr     |\n",
      "-------------------------------------------------\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 575s 306ms/step - loss: 0.0134 - acc: 0.9247 - val_loss: 0.0397 - val_acc: 0.8010\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 670s 357ms/step - loss: 0.0294 - acc: 0.8529 - val_loss: 0.0208 - val_acc: 0.8958\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 631s 336ms/step - loss: 0.0374 - acc: 0.8131 - val_loss: 0.0288 - val_acc: 0.8558\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 629s 335ms/step - loss: 0.0309 - acc: 0.8453 - val_loss: 0.0313 - val_acc: 0.8436\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 552s 294ms/step - loss: 0.0301 - acc: 0.8496 - val_loss: 0.0335 - val_acc: 0.8326\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.0372 - acc: 0.8141\n",
      "Test loss: 0.037187501043081286\n",
      "Test accuracy: 0.8140625\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 429s 229ms/step - loss: 0.0051 - acc: 0.9663 - val_loss: 0.0025 - val_acc: 0.9843\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 468s 250ms/step - loss: 0.0027 - acc: 0.9834 - val_loss: 0.0030 - val_acc: 0.9821\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 636s 339ms/step - loss: 0.0036 - acc: 0.9807 - val_loss: 0.0036 - val_acc: 0.9811\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 543s 289ms/step - loss: 0.0044 - acc: 0.9772 - val_loss: 0.0044 - val_acc: 0.9779\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 437s 233ms/step - loss: 0.0049 - acc: 0.9750 - val_loss: 0.0055 - val_acc: 0.9721\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.0080 - acc: 0.9594\n",
      "Test loss: 0.007987069408409297\n",
      "Test accuracy: 0.959375\n",
      "\n",
      "\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.9594  \u001b[0m | \u001b[95m 0.1     \u001b[0m | \u001b[95m 0.003093\u001b[0m |\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 442s 236ms/step - loss: 0.0052 - acc: 0.9659 - val_loss: 0.0019 - val_acc: 0.9872\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 473s 252ms/step - loss: 0.0019 - acc: 0.9881 - val_loss: 0.0017 - val_acc: 0.9881\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 438s 234ms/step - loss: 0.0014 - acc: 0.9913 - val_loss: 0.0018 - val_acc: 0.9889\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 442s 236ms/step - loss: 0.0011 - acc: 0.9930 - val_loss: 0.0017 - val_acc: 0.9898\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 516s 275ms/step - loss: 9.0166e-04 - acc: 0.9943 - val_loss: 0.0015 - val_acc: 0.9906\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.0019 - acc: 0.9891\n",
      "Test loss: 0.0018680369035791954\n",
      "Test accuracy: 0.9890625\n",
      "\n",
      "\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.9891  \u001b[0m | \u001b[95m 0.1587  \u001b[0m | \u001b[95m 0.001014\u001b[0m |\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 610s 325ms/step - loss: 0.0058 - acc: 0.9628 - val_loss: 0.0028 - val_acc: 0.9829\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 634s 338ms/step - loss: 0.0037 - acc: 0.9784 - val_loss: 0.0036 - val_acc: 0.9796\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 607s 324ms/step - loss: 0.0053 - acc: 0.9721 - val_loss: 0.0056 - val_acc: 0.9711\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 636s 339ms/step - loss: 0.0071 - acc: 0.9638 - val_loss: 0.0059 - val_acc: 0.9702\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 612s 326ms/step - loss: 0.0078 - acc: 0.9606 - val_loss: 0.0062 - val_acc: 0.9688\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.0075 - acc: 0.9625\n",
      "Test loss: 0.0074801745126023885\n",
      "Test accuracy: 0.9625\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 649s 346ms/step - loss: 0.0085 - acc: 0.9471 - val_loss: 0.0062 - val_acc: 0.9674\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 569s 303ms/step - loss: 0.0136 - acc: 0.9314 - val_loss: 0.0090 - val_acc: 0.9549\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 527s 281ms/step - loss: 0.0207 - acc: 0.8963 - val_loss: 0.0171 - val_acc: 0.9142\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 478s 255ms/step - loss: 0.0218 - acc: 0.8911 - val_loss: 0.0175 - val_acc: 0.9125\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 573s 306ms/step - loss: 0.0201 - acc: 0.8996 - val_loss: 0.0127 - val_acc: 0.9363\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.0128 - acc: 0.9359\n",
      "Test loss: 0.012812500307336449\n",
      "Test accuracy: 0.9359375\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 590s 315ms/step - loss: 0.0172 - acc: 0.9068 - val_loss: 0.0182 - val_acc: 0.9085\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 461s 246ms/step - loss: 0.0350 - acc: 0.8250 - val_loss: 0.0489 - val_acc: 0.7551\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 480s 256ms/step - loss: 0.0408 - acc: 0.7959 - val_loss: 0.0353 - val_acc: 0.8235\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 490s 261ms/step - loss: 0.0478 - acc: 0.7611 - val_loss: 0.0410 - val_acc: 0.7948\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 501s 267ms/step - loss: 0.0625 - acc: 0.6873 - val_loss: 0.0517 - val_acc: 0.7416\n",
      "10/10 [==============================] - 0s 41ms/step - loss: 0.0628 - acc: 0.6859\n",
      "Test loss: 0.0628124199807644\n",
      "Test accuracy: 0.6859375\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 475s 253ms/step - loss: 0.0198 - acc: 0.8934 - val_loss: 0.0351 - val_acc: 0.8242\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 488s 260ms/step - loss: 0.0493 - acc: 0.7532 - val_loss: 0.0378 - val_acc: 0.8110\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 470s 251ms/step - loss: 0.0468 - acc: 0.7659 - val_loss: 0.0664 - val_acc: 0.6680\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 537s 286ms/step - loss: 0.0901 - acc: 0.5497 - val_loss: 0.1770 - val_acc: 0.1152\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 574s 306ms/step - loss: 0.1000 - acc: 0.4999 - val_loss: 0.0331 - val_acc: 0.8346\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.0372 - acc: 0.8141\n",
      "Test loss: 0.03718746304512024\n",
      "Test accuracy: 0.8140625\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 502s 268ms/step - loss: 0.0129 - acc: 0.9278 - val_loss: 0.0302 - val_acc: 0.8484\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 502s 268ms/step - loss: 0.0261 - acc: 0.8694 - val_loss: 0.0172 - val_acc: 0.9140\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 470s 251ms/step - loss: 0.0438 - acc: 0.7810 - val_loss: 0.0383 - val_acc: 0.8084\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 458s 244ms/step - loss: 0.0459 - acc: 0.7704 - val_loss: 0.0529 - val_acc: 0.7357\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 447s 238ms/step - loss: 0.0560 - acc: 0.7201 - val_loss: 0.0374 - val_acc: 0.8128\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.0422 - acc: 0.7891\n",
      "Test loss: 0.04218750111758709\n",
      "Test accuracy: 0.7890625\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 557s 297ms/step - loss: 0.0084 - acc: 0.9490 - val_loss: 0.0082 - val_acc: 0.9579\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 438s 234ms/step - loss: 0.0200 - acc: 0.8994 - val_loss: 0.0124 - val_acc: 0.9381\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 479s 255ms/step - loss: 0.0249 - acc: 0.8755 - val_loss: 0.0217 - val_acc: 0.8915\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 468s 250ms/step - loss: 0.0265 - acc: 0.8676 - val_loss: 0.0147 - val_acc: 0.9266\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 455s 243ms/step - loss: 0.0257 - acc: 0.8712 - val_loss: 0.0213 - val_acc: 0.8937\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.0266 - acc: 0.8672\n",
      "Test loss: 0.026562500232830643\n",
      "Test accuracy: 0.8671875\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1466s 782ms/step - loss: 0.0050 - acc: 0.9670 - val_loss: 0.0020 - val_acc: 0.9865\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 442s 236ms/step - loss: 0.0022 - acc: 0.9857 - val_loss: 0.0021 - val_acc: 0.9870\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 444s 237ms/step - loss: 0.0017 - acc: 0.9897 - val_loss: 0.0019 - val_acc: 0.9888\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 441s 235ms/step - loss: 0.0017 - acc: 0.9901 - val_loss: 0.0021 - val_acc: 0.9881\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 437s 233ms/step - loss: 0.0017 - acc: 0.9903 - val_loss: 0.0021 - val_acc: 0.9881\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.0016 - acc: 0.9906\n",
      "Test loss: 0.0016050845061215569\n",
      "Test accuracy: 0.990625\n",
      "\n",
      "\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m 0.9906  \u001b[0m | \u001b[95m 0.1562  \u001b[0m | \u001b[95m 0.002061\u001b[0m |\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 508s 271ms/step - loss: 0.0495 - acc: 0.7454 - val_loss: 0.0498 - val_acc: 0.7510\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 447s 238ms/step - loss: 0.1321 - acc: 0.3393 - val_loss: 0.1804 - val_acc: 0.0982\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 443s 236ms/step - loss: 0.1804 - acc: 0.0978 - val_loss: 0.1804 - val_acc: 0.0982\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 449s 240ms/step - loss: 0.1805 - acc: 0.0974 - val_loss: 0.1804 - val_acc: 0.0982\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 449s 239ms/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1804 - val_acc: 0.0982\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.1784 - acc: 0.1078\n",
      "Test loss: 0.17843751311302186\n",
      "Test accuracy: 0.1078125\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 442s 236ms/step - loss: 0.0100 - acc: 0.9350 - val_loss: 0.0029 - val_acc: 0.9795\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 442s 236ms/step - loss: 0.0034 - acc: 0.9781 - val_loss: 0.0021 - val_acc: 0.9865\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 443s 236ms/step - loss: 0.0025 - acc: 0.9841 - val_loss: 0.0019 - val_acc: 0.9867\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 446s 238ms/step - loss: 0.0019 - acc: 0.9881 - val_loss: 0.0018 - val_acc: 0.9882\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 448s 239ms/step - loss: 0.0016 - acc: 0.9896 - val_loss: 0.0016 - val_acc: 0.9896\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.0013 - acc: 0.9937\n",
      "Test loss: 0.0013259890803965391\n",
      "Test accuracy: 0.99375\n",
      "\n",
      "\n",
      "| \u001b[95m 12      \u001b[0m | \u001b[95m 0.9937  \u001b[0m | \u001b[95m 0.3883  \u001b[0m | \u001b[95m 0.000207\u001b[0m |\n",
      "=================================================\n",
      "Iteration 0: \n",
      "\t{'target': 0.8140624761581421, 'params': {'dropout2_rate': 0.2668088018810296, 'lr': 0.007231212485077366}}\n",
      "Iteration 1: \n",
      "\t{'target': 0.9593750238418579, 'params': {'dropout2_rate': 0.10004574992693796, 'lr': 0.003093092469055214}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.989062488079071, 'params': {'dropout2_rate': 0.15870235632684523, 'lr': 0.0010141520882110983}}\n",
      "Iteration 3: \n",
      "\t{'target': 0.9624999761581421, 'params': {'dropout2_rate': 0.17450408455106836, 'lr': 0.003521051197726173}}\n",
      "Iteration 4: \n",
      "\t{'target': 0.9359375238418579, 'params': {'dropout2_rate': 0.258706989692268, 'lr': 0.005434285666633234}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.6859375238418579, 'params': {'dropout2_rate': 0.26767780576131794, 'lr': 0.00688367305392792}}\n",
      "Iteration 6: \n",
      "\t{'target': 0.8140624761581421, 'params': {'dropout2_rate': 0.18178089989260698, 'lr': 0.00879336262027036}}\n",
      "Iteration 7: \n",
      "\t{'target': 0.7890625, 'params': {'dropout2_rate': 0.11095503727917047, 'lr': 0.0067376283507661824}}\n",
      "Iteration 8: \n",
      "\t{'target': 0.8671875, 'params': {'dropout2_rate': 0.2669219209468508, 'lr': 0.005631029301612942}}\n",
      "Iteration 9: \n",
      "\t{'target': 0.9906250238418579, 'params': {'dropout2_rate': 0.15615477543809353, 'lr': 0.0020612047419403}}\n",
      "Iteration 10: \n",
      "\t{'target': 0.10781250149011612, 'params': {'dropout2_rate': 0.5, 'lr': 0.01}}\n",
      "Iteration 11: \n",
      "\t{'target': 0.9937499761581421, 'params': {'dropout2_rate': 0.3883228578249672, 'lr': 0.00020765186492235896}}\n",
      "{'target': 0.9937499761581421, 'params': {'dropout2_rate': 0.3883228578249672, 'lr': 0.00020765186492235896}}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f            = fit_with_partial,\n",
    "    pbounds      = bounds_2,\n",
    "    verbose      = 1,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points = 10, n_iter = 2,)\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': 0.9937499761581421, 'params': {'dropout2_rate': 0.3883228578249672, 'lr': 0.00020765186492235896}}\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/fmfn/BayesianOptimization\n",
    "- https://stackoverflow.com/questions/55586472/mnist-data-set-up-batch\n",
    "- https://keras.io/examples/mnist_cnn/\n",
    "- https://www.youtube.com/watch?v=sXdxyUCCm8s\n",
    "- https://machinelearningapplied.com/hyperparameter-search-with-bayesian-optimization-for-keras-cnn-classification-and-ensembling/\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "- https://stackoverflow.com/questions/55586472/mnist-data-set-up-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
